{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "defa46dd-e153-4c1b-b161-311df5755585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict  as edict\n",
    "import torch\n",
    "def create_args(exp_params):\n",
    "    args = edict()\n",
    "    task_args = edict()\n",
    "    args.exp_id=\"X\"\n",
    "    args.seed = exp_params['seed']\n",
    "    # --------------- MODEL ---------------------\n",
    "    args.model = 'scnn'\n",
    "    args.hidden_dim = 100\n",
    "    args.output_dims = 1  # Equals number of classes\n",
    "    \n",
    "    # ---------- TRAINING PARAMS ----------------\n",
    "    args.load_pretrained = True                                    # Do we start with a pretrained model or not \n",
    "    args.pretrained_path = f\"../models/scnn_{exp_params['method']}_{exp_params['dataset']}_{exp_params['corr']}_True_{exp_params['seed']}_best_.pth\" # path to pretrained model\n",
    "    task_args.n_interventions = 0                                  # Amount of times we stop training before applying intervention (forgetting, playing)\n",
    "    task_args.total_iterations = exp_params['max_iters']                         # 9452 = 1 epoch\n",
    "    \n",
    "    # ---------- MODEL PERSISTENCE --------------\n",
    "    args.save_model = False                                        # Save last model\n",
    "    args.save_best = False                                          # Save best performing model\n",
    "    args.save_stats = True                                         # Save final performance metrics\n",
    "    args.save_model_folder = 'models'                              # Folder where models are stored \n",
    "    args.save_grads_folder = 'grads'                               # Folder where gradients are saved to\n",
    "    args.save_model_path = \".pth\"                                  # suffix for saved model (name depends on model settings)\n",
    "    args.use_comet = False\n",
    "    \n",
    "    # ------------------- TRAINING METHOD ------------------------------\n",
    "    args.max_cur_iter = 0\n",
    "    args.task_iter = 0\n",
    "    args.mode = [\"task\"                                             # task = train on dataset defined in task_args \n",
    "                   #, 'play'                                        #  play = train on dataset defined in play_args \n",
    "                   #,'forget'                                       # forget = after training on task, forget using method defined in args.forget_method\n",
    "                       ]\n",
    "    args.base_method = \"erm\"                                      # gdro = group distributionally robust optimization\n",
    "                                                                    # rw = reweight losses\n",
    "                                                                    # erm = Empirical Risk Minimization \n",
    "    \n",
    "    # --------- DATASET -----------------------------------------------------------------------------\n",
    "    args.eval_datasets = dict()                                    # Which datasets to evaluate\n",
    "    args.task_datasets = dict()     \n",
    "    args.dataset_paths = {'synmnist': \"../../datasets/SynMNIST\",      # Path for each dataset\n",
    "                          'mnistcifar': \"../../datasets/MNISTCIFAR\"}\n",
    "    args.task_datasets['env1'] = {'name': exp_params['dataset'], 'corr': float(exp_params['corr'])\n",
    "                                  , 'splits': ['train', 'test'], 'bs': 10000, \"binarize\": True}\n",
    "    \n",
    "    # All datasets listed on eval_datasets will be evaluated. One dataset per key, however, each dataset may evaluate multiple splits.\n",
    "    for ds_id, ds in args.task_datasets.items():\n",
    "        args.eval_datasets[f'task_{ds_id}'] = ds \n",
    "    args.eval_datasets['eval'] = {'name': exp_params['dataset'], 'corr': 0.0, 'splits': ['val'], 'bs': 50000, \"binarize\": True}\n",
    "    # -------- METRICS -----------------------------------------------------------------------------\n",
    "    args.metrics = ['acc', 'loss','worst_group_loss', 'worst_group_acc', \"best_group_loss\", \"best_group_acc\"]\n",
    "    # --------------- Consolidate all settings on args --------------------\n",
    "    args.task_args = task_args\n",
    "    args.svdropout_p = 0.0\n",
    "    return args\n",
    "\n",
    "def load_model(model, weights_path):\n",
    "\n",
    "    w = torch.load(weights_path)\n",
    "    s_dict = w['model']\n",
    "    s_dict2 = dict()\n",
    "    for k, v in s_dict.items():\n",
    "        if k in ['fc.0.weight','fc.0.bias']:\n",
    "            s_dict2[k.replace(\"0.\",\"\")] = v\n",
    "        else:\n",
    "            s_dict2[k] = v\n",
    "        \n",
    "    model.load_state_dict(s_dict2, strict=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cc7f5ec-a616-49da-a4ba-2ba43a581d45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from torch.optim import Adam\n",
    "from tqdm.notebook import tqdm\n",
    "sys.path.append('/media/alain/Data/Tesis/spur/')\n",
    "from dataset import make_dataloaders\n",
    "from train import train,evaluate_splits, run_eval_iteration\n",
    "from models import create_model\n",
    "from numpy.random import choice\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from utils import update_metrics, save_stats\n",
    "# load a model\n",
    "# load balanced dataset\n",
    "# define dataset size (hyperparameter)\n",
    "# finetune model on balanced dataset\n",
    "# report metrics (worst_group_*, best_group_*, acc, loss)\n",
    "# create table with data at the method/dataset/spur/seed level then aggregate metho/dataset/spur\n",
    "\n",
    "def run_dfr_experiment(exp_params):\n",
    "    print(exp_params)\n",
    "    all_metrics = {'task_env1': dict(), 'eval': dict()}\n",
    "    args = create_args(exp_params)\n",
    "    for k in all_metrics.keys():\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            for m in args.metrics:\n",
    "                all_metrics[k][f\"{split}_{m}\"] = []\n",
    "    \n",
    "    # define args for dataloader\n",
    "    model=create_model(args).cuda()\n",
    "    model = load_model(model, args.pretrained_path).cuda()\n",
    "    opt = Adam(model.parameters(), lr=0.001)#),momentum=0.9,weight_decay=0.01)\n",
    "    # reload datasets\n",
    "    dls = make_dataloaders(args)\n",
    "    dl = dls['task']['env1']['test']              #train on balanced version of dataset\n",
    "    n_samples = len(dl.dataset)\n",
    "    print(n_samples)\n",
    "    random_indices = choice(n_samples, exp_params[\"ft_size\"])\n",
    "    dl = DataLoader(Subset(dl.dataset, indices=random_indices),batch_size=10000,shuffle=True) # Get subset of dataset\n",
    "    for i in tqdm(range(args.task_args.total_iterations),total=args.task_args.total_iterations):\n",
    "        model,_,_ = train(model,dl,opt,args)\n",
    "        metrics = evaluate_splits(model,dls['eval'],args,\"task\")\n",
    "        # accumulate metrics\n",
    "        for ds_name, m in metrics.items():\n",
    "            all_metrics[ds_name] = update_metrics(all_metrics[ds_name], m)\n",
    "    \n",
    "    return args, all_metrics # {\"worst_group}\n",
    "\n",
    "def run_jtt_experiment(exp_params):\n",
    "    print(exp_params)\n",
    "    all_metrics = {'task_env1': dict(), 'eval': dict()}\n",
    "    args = create_args(exp_params)\n",
    "    for k in all_metrics.keys():\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            for m in args.metrics:\n",
    "                all_metrics[k][f\"{split}_{m}\"] = []\n",
    "    \n",
    "    # define args for dataloader\n",
    "    model=create_model(args).cuda()\n",
    "    model = load_model(model, args.pretrained_path).cuda()\n",
    "    opt = Adam(model.parameters(), lr=0.001)#),momentum=0.9,weight_decay=0.01)\n",
    "    # reload datasets\n",
    "    dls = make_dataloaders(args)\n",
    "    dl = dls['task']['env1']['train']              #train on balanced version of dataset\n",
    "    n_samples = len(dl.dataset)\n",
    "    # run evaluation iteration on train set\n",
    "    for n_batch, (x, y, g) in enumerate(dl):\n",
    "        data = {'input': x, 'labels': y, 'groups': g}\n",
    "    logits = run_eval_iteration(data,model,args)\n",
    "    incorrect = (logits['logits'].squeeze() >0.5) != y\n",
    "    weights = torch.ones_like(incorrect).float().cuda()\n",
    "    weights[incorrect] = exp_params['lambda']\n",
    "    print(f\"N# of Incorrect samples is: {incorrect.sum():0d}/9000\")\n",
    "    for i in tqdm(range(args.task_args.total_iterations),total=args.task_args.total_iterations):\n",
    "        model,_,_ = train(model,dl,opt,args,weights=weights)\n",
    "        metrics = evaluate_splits(model,dls['eval'],args,\"task\")\n",
    "        # accumulate metrics\n",
    "        for ds_name, m in metrics.items():\n",
    "            all_metrics[ds_name] = update_metrics(all_metrics[ds_name], m)\n",
    "    \n",
    "    return args, all_metrics # {\"worst_group}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4bca295-32aa-4b4d-b629-14e9ada7d8d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f87ac243454ea68fb83a3b7802a307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'scnn', 'method': 'jtt200', 'dataset': 'mnistcifar', 'corr': '0.25', 'seed': '222', 'max_iters': 3, 'lambda': 5}\n",
      "N# of Incorrect samples is: 720/9000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c40413e30dff4a579a732084c7335038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#   0-task_env1-metric    test     train\n",
      "------------------------  -------  -------\n",
      "loss                      2.060    1.977\n",
      "best_group_loss           0.001    0.002\n",
      "worst_group_loss          4.236    4.292\n",
      "acc                       53.30%   53.73%\n",
      "best_group_acc            100.00%  99.96%\n",
      "worst_group_acc           6.56%    6.42%\n",
      "#   0-eval-metric    val\n",
      "-------------------  -------\n",
      "loss                 2.088\n",
      "best_group_loss      0.001\n",
      "worst_group_loss     4.390\n",
      "acc                  53.30%\n",
      "best_group_acc       100.00%\n",
      "worst_group_acc      3.52%\n",
      "\n",
      "#   1-task_env1-metric    test    train\n",
      "------------------------  ------  -------\n",
      "loss                      0.404   0.353\n",
      "best_group_loss           0.028   0.042\n",
      "worst_group_loss          0.838   0.837\n",
      "acc                       83.10%  84.54%\n",
      "best_group_acc            99.46%  99.07%\n",
      "worst_group_acc           64.59%  62.31%\n",
      "#   1-eval-metric    val\n",
      "-------------------  -------\n",
      "loss                 0.399\n",
      "best_group_loss      0.032\n",
      "worst_group_loss     0.879\n",
      "acc                  81.70%\n",
      "best_group_acc       100.00%\n",
      "worst_group_acc      58.59%\n",
      "\n",
      "#   2-task_env1-metric    test    train\n",
      "------------------------  ------  -------\n",
      "loss                      0.751   0.718\n",
      "best_group_loss           0.019   0.011\n",
      "worst_group_loss          1.755   1.796\n",
      "acc                       68.60%  69.16%\n",
      "best_group_acc            99.67%  100.00%\n",
      "worst_group_acc           29.26%  26.21%\n",
      "#   2-eval-metric    val\n",
      "-------------------  -------\n",
      "loss                 0.782\n",
      "best_group_loss      0.010\n",
      "worst_group_loss     1.897\n",
      "acc                  67.50%\n",
      "best_group_acc       100.00%\n",
      "worst_group_acc      23.11%\n",
      "jtt200_5\n",
      "{'model': 'scnn', 'method': 'jtt250', 'dataset': 'mnistcifar', 'corr': '0.5', 'seed': '333', 'max_iters': 3, 'lambda': 5}\n",
      "N# of Incorrect samples is: 601/9000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76aedd9aca9040489bf2ca86d0ecf13d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#   0-task_env1-metric    test     train\n",
      "------------------------  -------  -------\n",
      "loss                      3.196    2.557\n",
      "best_group_loss           0.001    0.000\n",
      "worst_group_loss          7.199    7.041\n",
      "acc                       50.90%   51.32%\n",
      "best_group_acc            100.00%  100.00%\n",
      "worst_group_acc           0.00%    0.00%\n",
      "#   0-eval-metric    val\n",
      "-------------------  -------\n",
      "loss                 3.023\n",
      "best_group_loss      0.000\n",
      "worst_group_loss     7.506\n",
      "acc                  51.10%\n",
      "best_group_acc       100.00%\n",
      "worst_group_acc      0.00%\n",
      "\n",
      "#   1-task_env1-metric    test    train\n",
      "------------------------  ------  -------\n",
      "loss                      0.507   0.288\n",
      "best_group_loss           0.021   0.019\n",
      "worst_group_loss          1.131   1.086\n",
      "acc                       78.80%  87.38%\n",
      "best_group_acc            99.29%  99.77%\n",
      "worst_group_acc           52.46%  48.60%\n",
      "#   1-eval-metric    val\n",
      "-------------------  -------\n",
      "loss                 0.444\n",
      "best_group_loss      0.016\n",
      "worst_group_loss     1.262\n",
      "acc                  80.40%\n",
      "best_group_acc       100.00%\n",
      "worst_group_acc      45.02%\n",
      "\n",
      "#   2-task_env1-metric    test     train\n",
      "------------------------  -------  -------\n",
      "loss                      0.621    0.370\n",
      "best_group_loss           0.011    0.018\n",
      "worst_group_loss          1.445    1.316\n",
      "acc                       75.50%   84.21%\n",
      "best_group_acc            100.00%  99.65%\n",
      "worst_group_acc           44.35%   44.84%\n",
      "#   2-eval-metric    val\n",
      "-------------------  -------\n",
      "loss                 0.548\n",
      "best_group_loss      0.011\n",
      "worst_group_loss     1.513\n",
      "acc                  78.50%\n",
      "best_group_acc       100.00%\n",
      "worst_group_acc      43.36%\n",
      "jtt250_5\n",
      "{'model': 'scnn', 'method': 'jtt100', 'dataset': 'mnistcifar', 'corr': '0.5', 'seed': '333', 'max_iters': 3, 'lambda': 5}\n",
      "N# of Incorrect samples is: 1192/9000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbc89c7d4e2c4a55b4ff4790640ef8e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#   0-task_env1-metric    test     train\n",
      "------------------------  -------  -------\n",
      "loss                      3.222    2.737\n",
      "best_group_loss           0.001    0.001\n",
      "worst_group_loss          7.039    6.938\n",
      "acc                       50.50%   50.20%\n",
      "best_group_acc            100.00%  100.00%\n",
      "worst_group_acc           0.00%    0.00%\n",
      "#   0-eval-metric    val\n",
      "-------------------  -------\n",
      "loss                 3.064\n",
      "best_group_loss      0.001\n",
      "worst_group_loss     7.157\n",
      "acc                  49.90%\n",
      "best_group_acc       100.00%\n",
      "worst_group_acc      0.00%\n",
      "\n",
      "#   1-task_env1-metric    test     train\n",
      "------------------------  -------  -------\n",
      "loss                      0.660    0.416\n",
      "best_group_loss           0.033    0.036\n",
      "worst_group_loss          1.410    1.433\n",
      "acc                       71.10%   81.43%\n",
      "best_group_acc            100.00%  99.77%\n",
      "worst_group_acc           36.89%   35.53%\n",
      "#   1-eval-metric    val\n",
      "-------------------  ------\n",
      "loss                 0.571\n",
      "best_group_loss      0.035\n",
      "worst_group_loss     1.485\n",
      "acc                  74.90%\n",
      "best_group_acc       99.58%\n",
      "worst_group_acc      34.26%\n",
      "\n",
      "#   2-task_env1-metric    test     train\n",
      "------------------------  -------  -------\n",
      "loss                      0.871    0.540\n",
      "best_group_loss           0.021    0.036\n",
      "worst_group_loss          1.978    1.887\n",
      "acc                       63.90%   75.64%\n",
      "best_group_acc            100.00%  99.21%\n",
      "worst_group_acc           19.83%   19.52%\n",
      "#   2-eval-metric    val\n",
      "-------------------  -------\n",
      "loss                 0.732\n",
      "best_group_loss      0.032\n",
      "worst_group_loss     1.999\n",
      "acc                  68.70%\n",
      "best_group_acc       100.00%\n",
      "worst_group_acc      19.53%\n",
      "jtt100_5\n",
      "{'model': 'scnn', 'method': 'jtt100', 'dataset': 'mnistcifar', 'corr': '0.0', 'seed': '111', 'max_iters': 3, 'lambda': 5}\n",
      "N# of Incorrect samples is: 1407/9000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "082a844b254c4b9e82a7f098e9ce6d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#   0-task_env1-metric    test     train\n",
      "------------------------  -------  -------\n",
      "loss                      2.302    2.307\n",
      "best_group_loss           0.006    0.006\n",
      "worst_group_loss          4.817    4.678\n",
      "acc                       50.40%   49.97%\n",
      "best_group_acc            100.00%  100.00%\n",
      "worst_group_acc           0.00%    0.04%\n",
      "#   0-eval-metric    val\n",
      "-------------------  -------\n",
      "loss                 2.372\n",
      "best_group_loss      0.006\n",
      "worst_group_loss     4.720\n",
      "acc                  49.60%\n",
      "best_group_acc       100.00%\n",
      "worst_group_acc      0.00%\n",
      "\n",
      "#   1-task_env1-metric    test    train\n",
      "------------------------  ------  -------\n",
      "loss                      0.548   0.586\n",
      "best_group_loss           0.067   0.073\n",
      "worst_group_loss          1.039   1.108\n",
      "acc                       73.30%  70.81%\n",
      "best_group_acc            99.60%  99.56%\n",
      "worst_group_acc           43.18%  41.70%\n",
      "#   1-eval-metric    val\n",
      "-------------------  ------\n",
      "loss                 0.586\n",
      "best_group_loss      0.072\n",
      "worst_group_loss     1.092\n",
      "acc                  71.20%\n",
      "best_group_acc       99.58%\n",
      "worst_group_acc      42.23%\n",
      "\n",
      "#   2-task_env1-metric    test    train\n",
      "------------------------  ------  -------\n",
      "loss                      0.541   0.543\n",
      "best_group_loss           0.060   0.092\n",
      "worst_group_loss          1.081   1.038\n",
      "acc                       75.20%  75.21%\n",
      "best_group_acc            98.11%  96.70%\n",
      "worst_group_acc           49.80%  52.20%\n",
      "#   2-eval-metric    val\n",
      "-------------------  ------\n",
      "loss                 0.537\n",
      "best_group_loss      0.066\n",
      "worst_group_loss     1.051\n",
      "acc                  75.10%\n",
      "best_group_acc       98.41%\n",
      "worst_group_acc      49.22%\n",
      "jtt100_5\n",
      "{'model': 'scnn', 'method': 'jtt100', 'dataset': 'mnistcifar', 'corr': '0.25', 'seed': '111', 'max_iters': 3, 'lambda': 5}\n",
      "N# of Incorrect samples is: 1172/9000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d06c2625f641e3bff596c60842b8a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#   0-task_env1-metric    test    train\n",
      "------------------------  ------  -------\n",
      "loss                      2.070   1.973\n",
      "best_group_loss           0.013   0.013\n",
      "worst_group_loss          4.281   4.359\n",
      "acc                       54.30%  54.32%\n",
      "best_group_acc            99.68%  99.23%\n",
      "worst_group_acc           8.20%   6.36%\n",
      "#   0-eval-metric    val\n",
      "-------------------  -------\n",
      "loss                 2.038\n",
      "best_group_loss      0.004\n",
      "worst_group_loss     4.421\n",
      "acc                  54.20%\n",
      "best_group_acc       100.00%\n",
      "worst_group_acc      3.91%\n",
      "\n",
      "#   1-task_env1-metric    test    train\n",
      "------------------------  ------  -------\n",
      "loss                      0.522   0.471\n",
      "best_group_loss           0.079   0.082\n",
      "worst_group_loss          1.066   1.080\n",
      "acc                       76.20%  78.98%\n",
      "best_group_acc            95.68%  97.01%\n",
      "worst_group_acc           50.82%  51.83%\n",
      "#   1-eval-metric    val\n",
      "-------------------  ------\n",
      "loss                 0.499\n",
      "best_group_loss      0.074\n",
      "worst_group_loss     1.099\n",
      "acc                  76.40%\n",
      "best_group_acc       97.63%\n",
      "worst_group_acc      46.88%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m e[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_iters\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m max_iters\n\u001b[1;32m     37\u001b[0m e[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlambda\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m lmbda\n\u001b[0;32m---> 38\u001b[0m args, results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_jtt_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m args\u001b[38;5;241m.\u001b[39mbase_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlmbda\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(args\u001b[38;5;241m.\u001b[39mbase_method)\n",
      "Cell \u001b[0;32mIn[2], line 73\u001b[0m, in \u001b[0;36mrun_jtt_experiment\u001b[0;34m(exp_params)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN# of Incorrect samples is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mincorrect\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m0d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/9000\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(args\u001b[38;5;241m.\u001b[39mtask_args\u001b[38;5;241m.\u001b[39mtotal_iterations),total\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mtask_args\u001b[38;5;241m.\u001b[39mtotal_iterations):\n\u001b[0;32m---> 73\u001b[0m     model,_,_ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdl\u001b[49m\u001b[43m,\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m evaluate_splits(model,dls[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m'\u001b[39m],args,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m# accumulate metrics\u001b[39;00m\n",
      "File \u001b[0;32m/media/alain/Data/Tesis/spur/train.py:102\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dl, opt, args, caption, return_grads, weights)\u001b[0m\n\u001b[1;32m    100\u001b[0m bs \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    101\u001b[0m data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m: x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: y, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroups\u001b[39m\u001b[38;5;124m'\u001b[39m: g}\n\u001b[0;32m--> 102\u001b[0m model, result \u001b[38;5;241m=\u001b[39m \u001b[43mrun_train_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Update on progress\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m#print(f\"\\r{n_batch+1}/{total_batches} ({100*(n_batch+1)/(total_batches):.2f}%)\", end=\"\")    \u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Do we need to stop prematurely?\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mmax_cur_iter \u001b[38;5;241m==\u001b[39m args[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_iter\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[0;32m/media/alain/Data/Tesis/spur/train.py:70\u001b[0m, in \u001b[0;36mrun_train_iteration\u001b[0;34m(data, model, opt, args, weights)\u001b[0m\n\u001b[1;32m     67\u001b[0m mean_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     68\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 70\u001b[0m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroup_loss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, metrics\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from os.path import join\n",
    "from os import listdir\n",
    "def choose_experiments(method, model_dir = \"models\"):\n",
    "    def make_file_dict(f):\n",
    "        f = f.split(\"_\")\n",
    "        return {\n",
    "                'model': f[0],\n",
    "                'method': f[1],\n",
    "                'dataset': f[2],\n",
    "                'corr': f[3],\n",
    "                'seed': f[5]\n",
    "               }\n",
    "    files = []\n",
    "    for f in listdir(model_dir):\n",
    "        if method in f:\n",
    "            files.append(make_file_dict(f))\n",
    "    return files\n",
    "\n",
    "exps = choose_experiments(\"jtt\", model_dir=\"../models\")\n",
    "max_iters = 3\n",
    "size_of_ft = 1000\n",
    "\n",
    "method = \"jtt\"\n",
    "\n",
    "if method == \"dfr\":\n",
    "    for size_of_ft in [10, 50, 100, 200, 500, 1000]:\n",
    "        for e in tqdm(exps,total=len(exps)):\n",
    "            e['max_iters'] = max_iters\n",
    "            e['ft_size'] = size_of_ft\n",
    "            args, results = run_dfr_experiment(e)\n",
    "            args.base_method +=f\"_ft_{size_of_ft}\"\n",
    "            save_stats(args,results,root=\"../stats\")\n",
    "elif method == \"jtt\":\n",
    "    for lmbda in [5,10,15,20]:\n",
    "        for e in tqdm(exps,total=len(exps)): \n",
    "            e['max_iters'] = max_iters\n",
    "            e['lambda'] = lmbda\n",
    "            args, results = run_jtt_experiment(e)\n",
    "            args.base_method = f\"{e['method']}_{lmbda}\"\n",
    "            print(args.base_method)\n",
    "            save_stats(args,results,root=\"../stats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ebb619-b614-4cf2-bb02-a4694a29e1e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
