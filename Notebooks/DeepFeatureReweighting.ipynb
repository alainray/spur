{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "defa46dd-e153-4c1b-b161-311df5755585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict  as edict\n",
    "import torch\n",
    "def create_args(exp_params):\n",
    "    args = edict()\n",
    "    task_args = edict()\n",
    "    args.exp_id=\"X\"\n",
    "    args.seed = exp_params['seed']\n",
    "    # --------------- MODEL ---------------------\n",
    "    args.model = 'scnn'\n",
    "    args.hidden_dim = 100\n",
    "    args.output_dims = 1  # Equals number of classes\n",
    "    \n",
    "    # ---------- TRAINING PARAMS ----------------\n",
    "    args.load_pretrained = True                                    # Do we start with a pretrained model or not \n",
    "    args.pretrained_path = f\"../models/scnn_{exp_params['method']}_{exp_params['dataset']}_{exp_params['corr']}_True_{exp_params['seed']}_best_.pth\" # path to pretrained model\n",
    "    task_args.n_interventions = 0                                  # Amount of times we stop training before applying intervention (forgetting, playing)\n",
    "    task_args.total_iterations = exp_params['max_iters']                         # 9452 = 1 epoch\n",
    "    \n",
    "    # ---------- MODEL PERSISTENCE --------------\n",
    "    args.save_model = False                                        # Save last model\n",
    "    args.save_best = False                                          # Save best performing model\n",
    "    args.save_stats = True                                         # Save final performance metrics\n",
    "    args.save_model_folder = 'models'                              # Folder where models are stored \n",
    "    args.save_grads_folder = 'grads'                               # Folder where gradients are saved to\n",
    "    args.save_model_path = \".pth\"                                  # suffix for saved model (name depends on model settings)\n",
    "    args.use_comet = False\n",
    "    \n",
    "    # ------------------- TRAINING METHOD ------------------------------\n",
    "    args.max_cur_iter = 0\n",
    "    args.task_iter = 0\n",
    "    args.mode = [\"task\"                                             # task = train on dataset defined in task_args \n",
    "                   #, 'play'                                        #  play = train on dataset defined in play_args \n",
    "                   #,'forget'                                       # forget = after training on task, forget using method defined in args.forget_method\n",
    "                       ]\n",
    "    args.base_method = \"erm\"                                      # gdro = group distributionally robust optimization\n",
    "                                                                    # rw = reweight losses\n",
    "                                                                    # erm = Empirical Risk Minimization \n",
    "    \n",
    "    # --------- DATASET -----------------------------------------------------------------------------\n",
    "    args.eval_datasets = dict()                                    # Which datasets to evaluate\n",
    "    args.task_datasets = dict()     \n",
    "    args.dataset_paths = {'synmnist': \"../../datasets/SynMNIST\",      # Path for each dataset\n",
    "                          'mnistcifar': \"../../datasets/MNISTCIFAR\"}\n",
    "    args.task_datasets['env1'] = {'name': exp_params['dataset'], 'corr': float(exp_params['corr'])\n",
    "                                  , 'splits': ['train', 'test'], 'bs': 10000, \"binarize\": True}\n",
    "    \n",
    "    # All datasets listed on eval_datasets will be evaluated. One dataset per key, however, each dataset may evaluate multiple splits.\n",
    "    for ds_id, ds in args.task_datasets.items():\n",
    "        args.eval_datasets[f'task_{ds_id}'] = ds \n",
    "    args.eval_datasets['eval'] = {'name': exp_params['dataset'], 'corr': 0.0, 'splits': ['val'], 'bs': 50000, \"binarize\": True}\n",
    "    # -------- METRICS -----------------------------------------------------------------------------\n",
    "    args.metrics = ['acc', 'loss','worst_group_loss', 'worst_group_acc', \"best_group_loss\", \"best_group_acc\"]\n",
    "    # --------------- Consolidate all settings on args --------------------\n",
    "    args.task_args = task_args\n",
    "    return args\n",
    "\n",
    "def load_model(model, weights_path):\n",
    "\n",
    "    w = torch.load(weights_path)\n",
    "    s_dict = w['model']\n",
    "    s_dict2 = dict()\n",
    "    for k, v in s_dict.items():\n",
    "        if k in ['fc.0.weight','fc.0.bias']:\n",
    "            s_dict2[k.replace(\"0.\",\"\")] = v\n",
    "        else:\n",
    "            s_dict2[k] = v\n",
    "        \n",
    "    model.load_state_dict(s_dict2, strict=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cc7f5ec-a616-49da-a4ba-2ba43a581d45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from torch.optim import Adam\n",
    "from tqdm.notebook import tqdm\n",
    "sys.path.append('/media/alain/Data/Tesis/spur/')\n",
    "from dataset import make_dataloaders\n",
    "from train import train,evaluate_splits\n",
    "from models import create_model\n",
    "from numpy.random import choice\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from utils import update_metrics, save_stats\n",
    "# load a model\n",
    "# load balanced dataset\n",
    "# define dataset size (hyperparameter)\n",
    "# finetune model on balanced dataset\n",
    "# report metrics (worst_group_*, best_group_*, acc, loss)\n",
    "# create table with data at the method/dataset/spur/seed level then aggregate metho/dataset/spur\n",
    "\n",
    "def run_experiment(exp_params):\n",
    "    print(exp_params)\n",
    "    all_metrics = {'task_env1': dict(), 'eval': dict()}\n",
    "    args = create_args(exp_params)\n",
    "    for k in all_metrics.keys():\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            for m in args.metrics:\n",
    "                all_metrics[k][f\"{split}_{m}\"] = []\n",
    "    \n",
    "    # define args for dataloader\n",
    "    model=create_model(args).cuda()\n",
    "    model = load_model(model, args.pretrained_path).cuda()\n",
    "    opt = Adam(model.parameters(), lr=0.001)#),momentum=0.9,weight_decay=0.01)\n",
    "    # reload datasets\n",
    "    dls = make_dataloaders(args)\n",
    "    dl = dls['task']['env1']['test']              #train on balanced version of dataset\n",
    "    n_samples = len(dl.dataset)\n",
    "    print(n_samples)\n",
    "    random_indices = choice(n_samples, exp_params[\"ft_size\"])\n",
    "    dl = DataLoader(Subset(dl.dataset, indices=random_indices),batch_size=10000,shuffle=True) # Get subset of dataset\n",
    "    for i in tqdm(range(args.task_args.total_iterations),total=args.task_args.total_iterations):\n",
    "        model,_,_ = train(model,dl,opt,args)\n",
    "        metrics = evaluate_splits(model,dls['eval'],args,\"task\")\n",
    "        # accumulate metrics\n",
    "        for ds_name, m in metrics.items():\n",
    "            all_metrics[ds_name] = update_metrics(all_metrics[ds_name], m)\n",
    "    \n",
    "    return args, all_metrics # {\"worst_group}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4bca295-32aa-4b4d-b629-14e9ada7d8d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32bc697d84dd4b889e06f5ce6ae28234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'scnn', 'method': 'rw', 'dataset': 'mnistcifar', 'corr': '0.5', 'seed': '777', 'max_iters': 3, 'ft_size': 1000}\n",
      "1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66ea12df50384561ad0f95603d4e38cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#   0-task_env1-metric    train    test\n",
      "------------------------  -------  ------\n",
      "loss                      1.105    1.359\n",
      "best_group_loss           0.004    0.049\n",
      "worst_group_loss          2.461    2.651\n",
      "acc                       86.43%   85.70%\n",
      "best_group_acc            99.72%   99.29%\n",
      "worst_group_acc           72.27%   71.54%\n",
      "#   0-eval-metric    val\n",
      "-------------------  ------\n",
      "loss                 1.549\n",
      "best_group_loss      0.060\n",
      "worst_group_loss     3.166\n",
      "acc                  84.30%\n",
      "best_group_acc       98.75%\n",
      "worst_group_acc      70.52%\n",
      "\n",
      "#   1-task_env1-metric    train    test\n",
      "------------------------  -------  -------\n",
      "loss                      2.808    3.271\n",
      "best_group_loss           0.000    0.001\n",
      "worst_group_loss          6.351    6.870\n",
      "acc                       76.23%   75.60%\n",
      "best_group_acc            100.00%  100.00%\n",
      "worst_group_acc           49.30%   47.52%\n",
      "#   1-eval-metric    val\n",
      "-------------------  -------\n",
      "loss                 3.283\n",
      "best_group_loss      0.002\n",
      "worst_group_loss     7.323\n",
      "acc                  75.40%\n",
      "best_group_acc       100.00%\n",
      "worst_group_acc      45.83%\n",
      "\n",
      "#   2-task_env1-metric    train    test\n",
      "------------------------  -------  ------\n",
      "loss                      0.989    1.178\n",
      "best_group_loss           0.000    0.103\n",
      "worst_group_loss          2.428    2.869\n",
      "acc                       87.13%   86.90%\n",
      "best_group_acc            100.00%  97.27%\n",
      "worst_group_acc           70.48%   70.92%\n",
      "#   2-eval-metric    val\n",
      "-------------------  ------\n",
      "loss                 1.252\n",
      "best_group_loss      0.109\n",
      "worst_group_loss     3.000\n",
      "acc                  86.60%\n",
      "best_group_acc       98.80%\n",
      "worst_group_acc      68.33%\n",
      "{'model': 'scnn', 'method': 'rw', 'dataset': 'mnistcifar', 'corr': '0.9', 'seed': '888', 'max_iters': 3, 'ft_size': 1000}\n",
      "1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b07dae12c7094fc5866bd9d04ac04bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#   0-task_env1-metric    train    test\n",
      "------------------------  -------  -------\n",
      "loss                      0.241    1.752\n",
      "best_group_loss           0.001    0.000\n",
      "worst_group_loss          0.351    1.906\n",
      "acc                       95.41%   81.70%\n",
      "best_group_acc            100.00%  100.00%\n",
      "worst_group_acc           93.85%   79.29%\n",
      "#   0-eval-metric    val\n",
      "-------------------  ------\n",
      "loss                 1.025\n",
      "best_group_loss      0.141\n",
      "worst_group_loss     1.828\n",
      "acc                  88.20%\n",
      "best_group_acc       96.67%\n",
      "worst_group_acc      80.86%\n",
      "\n",
      "#   1-task_env1-metric    train    test\n",
      "------------------------  -------  -------\n",
      "loss                      1.156    1.587\n",
      "best_group_loss           0.000    0.000\n",
      "worst_group_loss          1.987    2.793\n",
      "acc                       87.51%   84.70%\n",
      "best_group_acc            100.00%  100.00%\n",
      "worst_group_acc           79.73%   74.69%\n",
      "#   1-eval-metric    val\n",
      "-------------------  ------\n",
      "loss                 1.441\n",
      "best_group_loss      0.370\n",
      "worst_group_loss     2.970\n",
      "acc                  85.70%\n",
      "best_group_acc       95.26%\n",
      "worst_group_acc      74.61%\n",
      "\n",
      "#   2-task_env1-metric    train    test\n",
      "------------------------  -------  -------\n",
      "loss                      2.381    1.644\n",
      "best_group_loss           0.000    0.000\n",
      "worst_group_loss          4.890    4.412\n",
      "acc                       80.51%   84.60%\n",
      "best_group_acc            100.00%  100.00%\n",
      "worst_group_acc           61.53%   61.11%\n",
      "#   2-eval-metric    val\n",
      "-------------------  ------\n",
      "loss                 2.094\n",
      "best_group_loss      0.056\n",
      "worst_group_loss     5.059\n",
      "acc                  82.30%\n",
      "best_group_acc       98.75%\n",
      "worst_group_acc      62.06%\n",
      "{'model': 'scnn', 'method': 'rw', 'dataset': 'mnistcifar', 'corr': '0.25', 'seed': '777', 'max_iters': 3, 'ft_size': 1000}\n",
      "1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0eab10596b049b4b25f0295dd872a5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#   0-task_env1-metric    train    test\n",
      "------------------------  -------  -------\n",
      "loss                      1.971    2.410\n",
      "best_group_loss           0.000    0.002\n",
      "worst_group_loss          4.543    6.027\n",
      "acc                       77.40%   75.90%\n",
      "best_group_acc            100.00%  100.00%\n",
      "worst_group_acc           51.68%   45.73%\n",
      "#   0-eval-metric    val\n",
      "-------------------  -------\n",
      "loss                 2.685\n",
      "best_group_loss      0.000\n",
      "worst_group_loss     6.151\n",
      "acc                  76.60%\n",
      "best_group_acc       100.00%\n",
      "worst_group_acc      47.50%\n",
      "\n",
      "#   1-task_env1-metric    train    test\n",
      "------------------------  -------  ------\n",
      "loss                      0.379    0.671\n",
      "best_group_loss           0.000    0.054\n",
      "worst_group_loss          1.080    1.834\n",
      "acc                       91.84%   90.70%\n",
      "best_group_acc            100.00%  98.36%\n",
      "worst_group_acc           78.10%   78.38%\n",
      "#   1-eval-metric    val\n",
      "-------------------  ------\n",
      "loss                 0.867\n",
      "best_group_loss      0.139\n",
      "worst_group_loss     1.919\n",
      "acc                  88.50%\n",
      "best_group_acc       96.88%\n",
      "worst_group_acc      78.66%\n",
      "\n",
      "#   2-task_env1-metric    train    test\n",
      "------------------------  -------  -------\n",
      "loss                      1.520    1.620\n",
      "best_group_loss           0.000    0.001\n",
      "worst_group_loss          3.832    4.336\n",
      "acc                       80.77%   83.00%\n",
      "best_group_acc            100.00%  100.00%\n",
      "worst_group_acc           55.63%   54.05%\n",
      "#   2-eval-metric    val\n",
      "-------------------  -------\n",
      "loss                 2.043\n",
      "best_group_loss      0.003\n",
      "worst_group_loss     4.788\n",
      "acc                  80.50%\n",
      "best_group_acc       100.00%\n",
      "worst_group_acc      56.13%\n",
      "{'model': 'scnn', 'method': 'rw', 'dataset': 'mnistcifar', 'corr': '0.9', 'seed': '555', 'max_iters': 3, 'ft_size': 1000}\n",
      "1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f79473e143a8498d91b025b845f686f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#   0-task_env1-metric    train    test\n",
      "------------------------  -------  -------\n",
      "loss                      3.632    7.473\n",
      "best_group_loss           0.000    0.000\n",
      "worst_group_loss          8.045    15.152\n",
      "acc                       73.71%   61.50%\n",
      "best_group_acc            100.00%  100.00%\n",
      "worst_group_acc           30.19%   24.27%\n",
      "#   0-eval-metric    val\n",
      "-------------------  -------\n",
      "loss                 6.020\n",
      "best_group_loss      0.000\n",
      "worst_group_loss     15.875\n",
      "acc                  64.90%\n",
      "best_group_acc       100.00%\n",
      "worst_group_acc      17.58%\n",
      "\n",
      "#   1-task_env1-metric    train    test\n",
      "------------------------  -------  -------\n",
      "loss                      0.296    1.320\n",
      "best_group_loss           0.001    0.000\n",
      "worst_group_loss          0.423    1.387\n",
      "acc                       94.79%   85.10%\n",
      "best_group_acc            100.00%  100.00%\n",
      "worst_group_acc           92.73%   84.10%\n",
      "#   1-eval-metric    val\n",
      "-------------------  ------\n",
      "loss                 0.985\n",
      "best_group_loss      0.187\n",
      "worst_group_loss     1.739\n",
      "acc                  88.70%\n",
      "best_group_acc       95.83%\n",
      "worst_group_acc      82.81%\n",
      "\n",
      "#   2-task_env1-metric    train    test\n",
      "------------------------  -------  -------\n",
      "loss                      2.930    4.424\n",
      "best_group_loss           0.000    0.000\n",
      "worst_group_loss          5.950    8.987\n",
      "acc                       75.48%   69.30%\n",
      "best_group_acc            100.00%  100.00%\n",
      "worst_group_acc           48.11%   38.70%\n",
      "#   2-eval-metric    val\n",
      "-------------------  -------\n",
      "loss                 3.854\n",
      "best_group_loss      0.000\n",
      "worst_group_loss     9.569\n",
      "acc                  71.80%\n",
      "best_group_acc       100.00%\n",
      "worst_group_acc      32.27%\n",
      "{'model': 'scnn', 'method': 'rw', 'dataset': 'mnistcifar', 'corr': '1.0', 'seed': '888', 'max_iters': 3, 'ft_size': 1000}\n",
      "1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "957e3bf2db854650ab2ab74eb4f3173d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#   0-task_env1-metric    train    test\n",
      "------------------------  -------  ------\n",
      "loss                      0.002    27.955\n",
      "best_group_loss           0.000    13.301\n",
      "worst_group_loss          0.004    42.377\n",
      "acc                       99.92%   1.50%\n",
      "best_group_acc            100.00%  2.62%\n",
      "worst_group_acc           99.84%   0.40%\n",
      "#   0-eval-metric    val\n",
      "-------------------  -------\n",
      "loss                 14.104\n",
      "best_group_loss      0.000\n",
      "worst_group_loss     41.737\n",
      "acc                  49.80%\n",
      "best_group_acc       100.00%\n",
      "worst_group_acc      0.00%\n",
      "\n",
      "#   1-task_env1-metric    train    test\n",
      "------------------------  -------  ------\n",
      "loss                      0.023    22.195\n",
      "best_group_loss           0.000    8.001\n",
      "worst_group_loss          0.045    36.164\n",
      "acc                       99.33%   7.30%\n",
      "best_group_acc            100.00%  14.72%\n",
      "worst_group_acc           98.66%   0.00%\n",
      "#   1-eval-metric    val\n",
      "-------------------  -------\n",
      "loss                 11.257\n",
      "best_group_loss      0.000\n",
      "worst_group_loss     35.644\n",
      "acc                  53.40%\n",
      "best_group_acc       100.00%\n",
      "worst_group_acc      0.00%\n",
      "\n",
      "#   2-task_env1-metric    train    test\n",
      "------------------------  -------  ------\n",
      "loss                      0.011    17.216\n",
      "best_group_loss           0.001    8.803\n",
      "worst_group_loss          0.021    25.495\n",
      "acc                       99.64%   5.10%\n",
      "best_group_acc            99.96%   9.88%\n",
      "worst_group_acc           99.33%   0.40%\n",
      "#   2-eval-metric    val\n",
      "-------------------  -------\n",
      "loss                 8.744\n",
      "best_group_loss      0.002\n",
      "worst_group_loss     25.093\n",
      "acc                  52.40%\n",
      "best_group_acc       100.00%\n",
      "worst_group_acc      0.00%\n",
      "{'model': 'scnn', 'method': 'rw', 'dataset': 'mnistcifar', 'corr': '0.9', 'seed': '123', 'max_iters': 3, 'ft_size': 1000}\n",
      "1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aadfb8b88b14f8aba9a632df3ba6eb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#   0-task_env1-metric    train    test\n",
      "------------------------  -------  -------\n",
      "loss                      0.327    2.164\n",
      "best_group_loss           0.000    0.000\n",
      "worst_group_loss          0.668    3.910\n",
      "acc                       92.70%   72.90%\n",
      "best_group_acc            100.00%  100.00%\n",
      "worst_group_acc           84.43%   53.97%\n",
      "#   0-eval-metric    val\n",
      "-------------------  ------\n",
      "loss                 1.428\n",
      "best_group_loss      0.003\n",
      "worst_group_loss     4.192\n",
      "acc                  80.60%\n",
      "best_group_acc       99.58%\n",
      "worst_group_acc      50.20%\n",
      "\n",
      "#   1-task_env1-metric    train    test\n",
      "------------------------  -------  -------\n",
      "loss                      0.252    1.060\n",
      "best_group_loss           0.000    0.001\n",
      "worst_group_loss          0.339    1.252\n",
      "acc                       94.01%   83.90%\n",
      "best_group_acc            100.00%  100.00%\n",
      "worst_group_acc           92.43%   82.16%\n",
      "#   1-eval-metric    val\n",
      "-------------------  ------\n",
      "loss                 0.782\n",
      "best_group_loss      0.158\n",
      "worst_group_loss     1.436\n",
      "acc                  87.80%\n",
      "best_group_acc       95.42%\n",
      "worst_group_acc      79.69%\n",
      "\n",
      "#   2-task_env1-metric    train    test\n",
      "------------------------  -------  -------\n",
      "loss                      0.597    0.982\n",
      "best_group_loss           0.000    0.000\n",
      "worst_group_loss          0.940    1.674\n",
      "acc                       88.87%   86.30%\n",
      "best_group_acc            100.00%  100.00%\n",
      "worst_group_acc           83.12%   78.22%\n",
      "#   2-eval-metric    val\n",
      "-------------------  ------\n",
      "loss                 0.889\n",
      "best_group_loss      0.283\n",
      "worst_group_loss     1.906\n",
      "acc                  85.90%\n",
      "best_group_acc       93.68%\n",
      "worst_group_acc      76.56%\n",
      "{'model': 'scnn', 'method': 'rw', 'dataset': 'mnistcifar', 'corr': '0.9', 'seed': '666', 'max_iters': 3, 'ft_size': 1000}\n",
      "1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a5e7d784de42f48a0764be46bb99a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#   0-task_env1-metric    train    test\n",
      "------------------------  -------  -------\n",
      "loss                      0.188    1.946\n",
      "best_group_loss           0.000    0.000\n",
      "worst_group_loss          0.326    2.164\n",
      "acc                       96.03%   79.40%\n",
      "best_group_acc            100.00%  100.00%\n",
      "worst_group_acc           93.20%   77.41%\n",
      "#   0-eval-metric    val\n",
      "-------------------  ------\n",
      "loss                 1.157\n",
      "best_group_loss      0.140\n",
      "worst_group_loss     2.601\n",
      "acc                  87.60%\n",
      "best_group_acc       98.33%\n",
      "worst_group_acc      75.70%\n",
      "\n",
      "#   1-task_env1-metric    train    test\n",
      "------------------------  -------  -------\n",
      "loss                      2.557    4.044\n",
      "best_group_loss           0.000    0.000\n",
      "worst_group_loss          5.197    7.931\n",
      "acc                       79.32%   73.40%\n",
      "best_group_acc            100.00%  100.00%\n",
      "worst_group_acc           59.25%   49.38%\n",
      "#   1-eval-metric    val\n",
      "-------------------  ------\n",
      "loss                 3.401\n",
      "best_group_loss      0.026\n",
      "worst_group_loss     7.536\n",
      "acc                  76.20%\n",
      "best_group_acc       98.42%\n",
      "worst_group_acc      50.00%\n",
      "\n",
      "#   2-task_env1-metric    train    test\n",
      "------------------------  -------  ------\n",
      "loss                      0.910    1.020\n",
      "best_group_loss           0.002    0.103\n",
      "worst_group_loss          1.458    1.193\n",
      "acc                       88.88%   89.50%\n",
      "best_group_acc            100.00%  95.45%\n",
      "worst_group_acc           83.95%   88.89%\n",
      "#   2-eval-metric    val\n",
      "-------------------  ------\n",
      "loss                 0.994\n",
      "best_group_loss      0.527\n",
      "worst_group_loss     1.390\n",
      "acc                  88.90%\n",
      "best_group_acc       92.97%\n",
      "worst_group_acc      84.86%\n",
      "{'model': 'scnn', 'method': 'rw', 'dataset': 'mnistcifar', 'corr': '0.0', 'seed': '333', 'max_iters': 3, 'ft_size': 1000}\n",
      "1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c06c9f5de57144a9ae7361968162e598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#   0-task_env1-metric    train    test\n",
      "------------------------  -------  ------\n",
      "loss                      0.574    0.987\n",
      "best_group_loss           0.010    0.096\n",
      "worst_group_loss          1.157    1.997\n",
      "acc                       89.07%   84.90%\n",
      "best_group_acc            99.69%   97.41%\n",
      "worst_group_acc           78.17%   72.29%\n",
      "#   0-eval-metric    val\n",
      "-------------------  ------\n",
      "loss                 1.352\n",
      "best_group_loss      0.148\n",
      "worst_group_loss     2.760\n",
      "acc                  84.00%\n",
      "best_group_acc       97.63%\n",
      "worst_group_acc      68.75%\n",
      "\n",
      "#   1-task_env1-metric    train    test\n",
      "------------------------  -------  ------\n",
      "loss                      2.560    2.924\n",
      "best_group_loss           0.000    0.009\n",
      "worst_group_loss          5.185    6.185\n",
      "acc                       68.56%   67.50%\n",
      "best_group_acc            100.00%  99.61%\n",
      "worst_group_acc           36.23%   32.20%\n",
      "#   1-eval-metric    val\n",
      "-------------------  ------\n",
      "loss                 3.139\n",
      "best_group_loss      0.020\n",
      "worst_group_loss     6.547\n",
      "acc                  66.60%\n",
      "best_group_acc       99.61%\n",
      "worst_group_acc      29.88%\n",
      "\n",
      "#   2-task_env1-metric    train    test\n",
      "------------------------  -------  ------\n",
      "loss                      0.088    0.400\n",
      "best_group_loss           0.005    0.151\n",
      "worst_group_loss          0.177    0.574\n",
      "acc                       97.38%   92.30%\n",
      "best_group_acc            99.87%   96.08%\n",
      "worst_group_acc           94.51%   89.22%\n",
      "#   2-eval-metric    val\n",
      "-------------------  ------\n",
      "loss                 0.575\n",
      "best_group_loss      0.366\n",
      "worst_group_loss     0.672\n",
      "acc                  91.30%\n",
      "best_group_acc       96.25%\n",
      "worst_group_acc      88.14%\n",
      "{'model': 'scnn', 'method': 'rw', 'dataset': 'mnistcifar', 'corr': '0.5', 'seed': '111', 'max_iters': 3, 'ft_size': 1000}\n",
      "1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fd73ebf666e4adf9c75bb25bab909ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#   0-task_env1-metric    train    test\n",
      "------------------------  -------  -------\n",
      "loss                      4.447    5.094\n",
      "best_group_loss           0.000    0.000\n",
      "worst_group_loss          8.955    10.594\n",
      "acc                       60.99%   60.90%\n",
      "best_group_acc            100.00%  100.00%\n",
      "worst_group_acc           19.30%   18.85%\n",
      "#   0-eval-metric    val\n",
      "-------------------  -------\n",
      "loss                 5.110\n",
      "best_group_loss      0.000\n",
      "worst_group_loss     10.686\n",
      "acc                  59.60%\n",
      "best_group_acc       100.00%\n",
      "worst_group_acc      14.74%\n",
      "\n",
      "#   1-task_env1-metric    train    test\n",
      "------------------------  -------  ------\n",
      "loss                      0.506    1.027\n",
      "best_group_loss           0.000    0.117\n",
      "worst_group_loss          1.219    2.006\n",
      "acc                       90.16%   86.10%\n",
      "best_group_acc            100.00%  98.09%\n",
      "worst_group_acc           77.35%   74.66%\n",
      "#   1-eval-metric    val\n",
      "-------------------  ------\n",
      "loss                 1.039\n",
      "best_group_loss      0.102\n",
      "worst_group_loss     2.189\n",
      "acc                  86.30%\n",
      "best_group_acc       98.41%\n",
      "worst_group_acc      73.83%\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m e[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_iters\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m max_iters\n\u001b[1;32m     25\u001b[0m e[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mft_size\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m size_of_ft\n\u001b[0;32m---> 26\u001b[0m args, results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m args\u001b[38;5;241m.\u001b[39mbase_method \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_ft_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize_of_ft\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m save_stats(args,results,root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../stats\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 40\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(exp_params)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(args\u001b[38;5;241m.\u001b[39mtask_args\u001b[38;5;241m.\u001b[39mtotal_iterations),total\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mtask_args\u001b[38;5;241m.\u001b[39mtotal_iterations):\n\u001b[1;32m     39\u001b[0m     model,_,_ \u001b[38;5;241m=\u001b[39m train(model,dl,opt,args)\n\u001b[0;32m---> 40\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_splits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdls\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# accumulate metrics\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ds_name, m \u001b[38;5;129;01min\u001b[39;00m metrics\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/media/alain/Data/Tesis/spur/train.py:127\u001b[0m, in \u001b[0;36mevaluate_splits\u001b[0;34m(model, dls, args, stage)\u001b[0m\n\u001b[1;32m    124\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split, dl \u001b[38;5;129;01min\u001b[39;00m ds\u001b[38;5;241m.\u001b[39mitems(): \n\u001b[0;32m--> 127\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m metrics\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    129\u001b[0m         results[k] \u001b[38;5;241m=\u001b[39m v\n",
      "File \u001b[0;32m/media/alain/Data/Tesis/spur/train.py:147\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(args, model, dl, caption, show)\u001b[0m\n\u001b[1;32m    144\u001b[0m evaluate_result \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m'\u001b[39m: []}\n\u001b[1;32m    145\u001b[0m evaluate_data\u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroups\u001b[39m\u001b[38;5;124m'\u001b[39m: []}\n\u001b[0;32m--> 147\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n_batch, (x, y, g) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dl):\n\u001b[1;32m    148\u001b[0m     data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m: x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: y, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroups\u001b[39m\u001b[38;5;124m'\u001b[39m: g}\n\u001b[1;32m    149\u001b[0m     result \u001b[38;5;241m=\u001b[39m run_eval_iteration(data, model, args)\n",
      "File \u001b[0;32m/media/alain/Data/Tesis/torch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/media/alain/Data/Tesis/torch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/media/alain/Data/Tesis/torch/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/alain/Data/Tesis/torch/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:175\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    172\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [default_collate(samples) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/media/alain/Data/Tesis/torch/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:175\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    172\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mdefault_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/media/alain/Data/Tesis/torch/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:141\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    139\u001b[0m         storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mstorage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    140\u001b[0m         out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstr_\u001b[39m\u001b[38;5;124m'\u001b[39m \\\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstring_\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mndarray\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmemmap\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;66;03m# array of string classes and object\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from os.path import join\n",
    "from os import listdir\n",
    "def choose_experiments(method, model_dir = \"models\"):\n",
    "    def make_file_dict(f):\n",
    "        f = f.split(\"_\")\n",
    "        return {\n",
    "                'model': f[0],\n",
    "                'method': f[1],\n",
    "                'dataset': f[2],\n",
    "                'corr': f[3],\n",
    "                'seed': f[5]\n",
    "               }\n",
    "    files = []\n",
    "    for f in listdir(model_dir):\n",
    "        if method in f:\n",
    "            files.append(make_file_dict(f))\n",
    "    return files\n",
    "\n",
    "exps = choose_experiments(\"rw\", model_dir=\"../models\")\n",
    "max_iters = 3\n",
    "size_of_ft = 1000\n",
    "for size_of_ft in [10, 50, 100, 200, 500, 1000]:\n",
    "    for e in tqdm(exps,total=len(exps)):\n",
    "        e['max_iters'] = max_iters\n",
    "        e['ft_size'] = size_of_ft\n",
    "        args, results = run_experiment(e)\n",
    "        args.base_method +=f\"_ft_{size_of_ft}\"\n",
    "        save_stats(args,results,root=\"../stats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf76c64a-2d3b-46a0-b4ba-f5307041034f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'../models/scnn_gdro_mnistcifar_0.75_True_222_best_.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f19e828d-c655-46a6-a1f3-3a85dcadc629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'scnn',\n",
       " 'method': 'mnistcifar',\n",
       " 'dataset': '0.75',\n",
       " 'corr': 'True',\n",
       " 'seed': 'nofrz',\n",
       " 'max_iters': 10}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4dc6692a-8223-4fa1-88c2-8f520053dc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/scnn_gdro_mnistcifar_0.75_True_222_best_.pth\n"
     ]
    }
   ],
   "source": [
    "!ls ../models/scnn_gdro_mnistcifar_0.75_True_222_best_.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6d3bc5fd-97db-4473-973e-e19301b7ebb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.load(\"../models/scnn_gdro_mnistcifar_0.75_True_222_best_.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "65f89ed0-38fa-42e7-8a10-af28be76915b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "011181f2-0101-4a8d-a446-0bad9d04fd69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff6217ee-a5d8-4aed-9c91-317838880ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choice(2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ea7bf4-ebc6-4307-80c3-7572ab8459a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
